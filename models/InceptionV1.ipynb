{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOTS5m3f_cON"
      },
      "source": [
        "### Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxtDWpGD_cOO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "from tqdm import notebook as tqdm\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.transforms import RandomVerticalFlip\n",
        "from torchvision.transforms import RandomHorizontalFlip\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qib0Py_1_cOO"
      },
      "source": [
        "## Loading and Pre Processing Data\n",
        "\n",
        "Resizing the images (so they each have the same amount of pixels, not to overfit to larger images) and converting them to Tensor, so they can be used in the model.\n",
        "\n",
        "Import the data from Google Drive as we assume that this is running on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV5Neyg4_cOP"
      },
      "outputs": [],
      "source": [
        "means = [0.43766510486602783, 0.49804747104644775, 0.3756938874721527]\n",
        "stds = [0.16779577732086182, 0.1552586406469345, 0.1632111817598343]\n",
        "transform_train = Compose([RandomHorizontalFlip(), RandomVerticalFlip(), Resize(size=(64,64)), ToTensor(),Normalize(mean=means, std=stds)])\n",
        "transform_val = Compose([Resize(size=(64,64)), ToTensor(),Normalize(mean=means, std=stds)])\n",
        "transform_test = Compose([Resize(size=(64,64)), ToTensor(),Normalize(mean=means, std=stds)])\n",
        "\n",
        "train_dataset = ImageFolder(root=\"/content/drive/MyDrive/splitted_data/train\",transform=transform_train)\n",
        "val_dataset = ImageFolder(root=\"/content/drive/MyDrive/splitted_data/val\",transform=transform_val)\n",
        "test_dataset = ImageFolder(root=\"/content/drive/MyDrive/splitted_data/test\",transform=transform_test)\n",
        "diffBg_dataset = ImageFolder(root=\"/content/drive/MyDrive/diffBg\",transform=transform_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1v9Eub_cOP"
      },
      "source": [
        "### Use a GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2xo6E8M_cOP",
        "outputId": "d2421910-2bd1-4527-8b4d-2a4440b2b816"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULTmc88_cOQ"
      },
      "source": [
        "## Defining the InceptionV1 Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEZ8yS1N_cOQ"
      },
      "outputs": [],
      "source": [
        "def layer_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    classname = classname.lower()\n",
        "    if classname.find('conv') != -1 or classname.find('linear') != -1:\n",
        "        gain = nn.init.calculate_gain(classname)\n",
        "        nn.init.xavier_uniform(m.weight, gain=gain)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant(m.bias, 0)\n",
        "    elif classname.find('batchnorm') != -1:\n",
        "        nn.init.constant(m.weight, 1)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant(m.bias, 0)\n",
        "    elif classname.find('embedding') != -1:\n",
        "        num_columns = m.weight.size(1)\n",
        "        sigma = 1/(num_columns**0.5)\n",
        "        m.weight.data.normal_(0, sigma).clamp_(-3*sigma, 3*sigma)\n",
        "\n",
        "class LRN(nn.Module):\n",
        "    def __init__(self, local_size=1, alpha=1.0, beta=0.75, k=1, ACROSS_CHANNELS=False):\n",
        "        super(LRN, self).__init__()\n",
        "        self.ACROSS_CHANNELS = ACROSS_CHANNELS\n",
        "        if ACROSS_CHANNELS:\n",
        "            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1),\n",
        "                    stride=1,\n",
        "                    padding=(int((local_size-1.0)/2), 0, 0))\n",
        "        else:\n",
        "            self.average=nn.AvgPool2d(kernel_size=local_size,\n",
        "                    stride=1,\n",
        "                    padding=int((local_size-1.0)/2))\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.ACROSS_CHANNELS:\n",
        "            div = x.pow(2).unsqueeze(1)\n",
        "            div = self.average(div).squeeze(1)\n",
        "            div = div.mul(self.alpha).add(self.k).pow(self.beta)\n",
        "        else:\n",
        "            div = x.pow(2)\n",
        "            div = self.average(div)\n",
        "            div = div.mul(self.alpha).add(self.k).pow(self.beta)\n",
        "        x = x.div(div)\n",
        "        return x\n",
        "\n",
        "class Inception_base(nn.Module):\n",
        "    def __init__(self, depth_dim, input_size, config):\n",
        "        super(Inception_base, self).__init__()\n",
        "\n",
        "        self.depth_dim = depth_dim\n",
        "\n",
        "        #mixed 'name'_1x1\n",
        "        self.conv1 = nn.Conv2d(input_size, out_channels=config[0][0], kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #mixed 'name'_3x3_bottleneck\n",
        "        self.conv3_1 = nn.Conv2d(input_size, out_channels=config[1][0], kernel_size=1, stride=1, padding=0)\n",
        "        #mixed 'name'_3x3\n",
        "        self.conv3_3 = nn.Conv2d(config[1][0], config[1][1], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # mixed 'name'_5x5_bottleneck\n",
        "        self.conv5_1 = nn.Conv2d(input_size, out_channels=config[2][0], kernel_size=1, stride=1, padding=0)\n",
        "        # mixed 'name'_5x5\n",
        "        self.conv5_5 = nn.Conv2d(config[2][0], config[2][1], kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        self.max_pool_1 = nn.MaxPool2d(kernel_size=config[3][0], stride=1, padding=1)\n",
        "        #mixed 'name'_pool_reduce\n",
        "        self.conv_max_1 = nn.Conv2d(input_size, out_channels=config[3][1], kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.apply(layer_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output1 = F.relu(self.conv1(input))\n",
        "\n",
        "        output2 = F.relu(self.conv3_1(input))\n",
        "        output2 = F.relu(self.conv3_3(output2))\n",
        "\n",
        "        output3 = F.relu(self.conv5_1(input))\n",
        "        output3 = F.relu(self.conv5_5(output3))\n",
        "\n",
        "        output4 = F.relu(self.conv_max_1(self.max_pool_1(input)))\n",
        "\n",
        "        return torch.cat([output1, output2, output3, output4], dim=self.depth_dim)\n",
        "\n",
        "class InceptionV1(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(InceptionV1, self).__init__()\n",
        "\n",
        "        #conv2d0\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.lrn1 = LRN(local_size=11, alpha=0.00109999999404, beta=0.5, k=2)\n",
        "\n",
        "        #conv2d1\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #conv2d2\n",
        "        self.conv3  = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "        self.lrn3 = LRN(local_size=11, alpha=0.00109999999404, beta=0.5, k=2)\n",
        "        self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception_3a = Inception_base(1, 192, [[64], [96,128], [16, 32], [3, 32]]) #3a\n",
        "        self.inception_3b = Inception_base(1, 256, [[128], [128,192], [32, 96], [3, 64]]) #3b\n",
        "        self.max_pool_inc3= nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "        self.inception_4a = Inception_base(1, 480, [[192], [ 96,204], [16, 48], [3, 64]]) #4a\n",
        "        self.inception_4b = Inception_base(1, 508, [[160], [112,224], [24, 64], [3, 64]]) #4b\n",
        "        self.inception_4c = Inception_base(1, 512, [[128], [128,256], [24, 64], [3, 64]]) #4c\n",
        "        self.inception_4d = Inception_base(1, 512, [[112], [144,288], [32, 64], [3, 64]]) #4d\n",
        "        self.inception_4e = Inception_base(1, 528, [[256], [160,320], [32,128], [3,128]]) #4e\n",
        "        self.max_pool_inc4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception_5a = Inception_base(1, 832, [[256], [160,320], [48,128], [3,128]]) #5a\n",
        "        self.inception_5b = Inception_base(1, 832, [[384], [192,384], [48,128], [3,128]]) #5b\n",
        "        self.avg_pool5 = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "        self.apply(layer_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output = self.max_pool1(F.relu(self.conv1(input)))\n",
        "        output = self.lrn1(output)\n",
        "\n",
        "        output = F.relu(self.conv2(output))\n",
        "        output = F.relu(self.conv3(output))\n",
        "        output = self.max_pool3(self.lrn3(output))\n",
        "\n",
        "        output = self.inception_3a(output)\n",
        "        output = self.inception_3b(output)\n",
        "        output = self.max_pool_inc3(output)\n",
        "\n",
        "        output = self.inception_4a(output)\n",
        "        output = self.inception_4b(output)\n",
        "        output = self.inception_4c(output)\n",
        "        output = self.inception_4d(output)\n",
        "        output = self.inception_4e(output)\n",
        "        output = self.max_pool_inc4(output)\n",
        "\n",
        "        output = self.inception_5a(output)\n",
        "        output = self.inception_5b(output)\n",
        "        output = self.avg_pool5(output)\n",
        "\n",
        "        output = output.view(-1, 1024)\n",
        "\n",
        "        if self.fc is not None:\n",
        "            output = self.dropout_layer(output)\n",
        "            output = self.fc(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform a sanity check for the correctness of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = InceptionV1()\n",
        "net = net.to(device)\n",
        "data = torch.randn(5,3,64,64)\n",
        "data = data.to(device)\n",
        "out = net.forward(data)\n",
        "assert(out.detach().cpu().numpy().shape == (5,4))\n",
        "print(\"Forward pass successful, shape matches\")\n",
        "\n",
        "for layer in net.children():\n",
        "  print(layer)\n",
        "  print(sum(p.numel() for p in layer.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ZESNKI_cOQ"
      },
      "source": [
        "### Move data and the model to the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddi0gqrJ_cOQ",
        "outputId": "c51d444c-06ea-4ab4-e43d-52c5fe550bb6"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
        "test_dl = DataLoader(test_dataset, batch_size=16)\n",
        "diffBg_dl = DataLoader(diffBg_dataset, batch_size=16)\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "INITIAL_LR = 0.1 # initial learning rate\n",
        "MOMENTUM = 0.9 # momentum for optimizer\n",
        "\n",
        "REG = 1e-3 # L2 regularization strength\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "\n",
        "# Add optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay = REG)\n",
        "EPOCHS = 30\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY_EPOCHS = 10\n",
        "DECAY = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for i in range(0, EPOCHS):\n",
        "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    net.train() # switch to train mode\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    # 1 epoch training\n",
        "    for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(train_dl), \"training...\"):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = net.forward(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1) \n",
        "        correct = predicted.eq(targets).sum()\n",
        "\n",
        "        correct_examples += correct\n",
        "        total_examples += len(targets)\n",
        "        train_loss += loss\n",
        "\n",
        "    avg_loss = train_loss / len(train_dl)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "    \n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['train_acc'].append(avg_acc)\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(val_dl), \"validating...\"):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = net.forward(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct = predicted.eq(targets).sum()\n",
        "\n",
        "            correct_examples += correct\n",
        "            total_examples += len(targets)\n",
        "            val_loss += loss\n",
        "\n",
        "    avg_loss = val_loss / len(val_dl)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "\n",
        "    history['val_loss'].append(avg_loss)\n",
        "    history['val_acc'].append(avg_acc)\n",
        "\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "           os.makedirs(CHECKPOINT_FOLDER)\n",
        "        print(\"Saving ...\")\n",
        "        state = {'state_dict': net.state_dict(),\n",
        "                'epoch': i,\n",
        "                'lr': current_learning_rate}\n",
        "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'inceptionv1.pth'))\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"Optimization finished: best validation accuracy is {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show the plots of the training and validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "V7ODc8NG_cOR",
        "outputId": "eb3dd695-583a-4277-fa97-5daf382e7f9d"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(history):\n",
        "  accuracies = [x.cpu().item() for x in history['val_acc']]\n",
        "  plt.plot(accuracies,'-x')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(\"Accuracy Vs No. of Epochs\")\n",
        "\n",
        "plot_accuracies(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_losses(history):\n",
        "  train_losses = [x.cpu().item() for x in history['train_loss']]\n",
        "  val_losses = [x.cpu().item() for x in history['val_loss']]\n",
        "  plt.plot(train_losses,'-bx')\n",
        "  plt.plot(val_losses,'-rx')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(['Training','Validation'])\n",
        "  plt.title(\"Loss Vs No. of Epochs\")\n",
        "plot_losses(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "n2cyP8TJkvIw",
        "outputId": "3a7a11f7-703b-4321-904f-dbe0fc0c6fee"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm.tqdm(test_dl, \"evaluating...\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "conf_mat = confusion_matrix(true_labels,predicted_labels)\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Reds\", cbar=False, xticklabels=class_names,yticklabels=class_names)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Modelling Corn Disease')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional testing on images with different backgrounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm.tqdm(diffBg_dl, \"evaluating...\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "conf_mat = confusion_matrix(true_labels,predicted_labels)\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Reds\", cbar=False, xticklabels=class_names,yticklabels=class_names)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Diff BG')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
